---
layout: default
title: 1. Overview of embeddings based retrieval
nav_order: 2
description: "Chroma for RAG"
has_children: true
parent:  Advanced Retrieval for AI - Chroma
---

Exploring the elements in an embeddings-based retrieval system reveals how it integrates seamlessly into a retrieval augmented generation loop with a language learning model (LLM). Let's dive into how this system operates in practice. The process starts when a user query is received, which interacts with a set of documents previously embedded and stored in the retrieval system, such as Chroma. This query is then processed through the same embedding model used for the documents, creating an embedding for the query itself. The retrieval system utilizes this embedding to identify the most relevant documents by finding those with the nearest neighbor embeddings.

<img src="/deeplearningai/advanced-retrieval-for-ai-chroma/images/Screenshot_2024-02-23_at_10.47.41 PM.png" width="80%" />


Following this, both the query and the identified relevant documents are returned to the LLM. The LLM then synthesizes the information from these documents to formulate an answer, showcasing the practical application of retrieval augmented generation. To illustrate this further, we employ helper functions from our utilities, including a basic word wrap function to enhance the readability of the documents. 

## Overview of embeddings-based retrieval

For our example, we opt to read from a PDF, specifically Microsoft's 2022 annual report, using a straightforward PDF Reader package that is open-source and easy to import. 



```python
from helper_utils import word_wrap
```

The process involves extracting text from each page of the report with this application, ensuring that we strip away any whitespace characters and omit any empty strings or pages from being sent to our retrieval system. This meticulous approach ensures that only relevant and content-rich pages are considered, enhancing the efficiency and accuracy of the retrieval process.

```python
from pypdf import PdfReader

reader = PdfReader("microsoft_annual_report_2022.pdf")
pdf_texts = [p.extract_text().strip() for p in reader.pages]

# Filter the empty strings
pdf_texts = [text for text in pdf_texts if text]

print(word_wrap(pdf_texts[0]))
```

    1 Dear shareholders, colleagues, customers, and partners:  
    We are
    ...
    this past fiscal year I have had the privilege to witness our customers
    use our platforms and tools to connect 
    what technology can do with
    what the world needs  it to do.  
    Here are just a few examples:  
  
    
    increasing access to digital skills. This year alone, more than 23 
    million people accessed digital skills training as part of 
    our global
    skills initiative.


You can view the pdf in your browser [here](./microsoft_annual_report_2022.pdf) if you would like. 

In our next step, we need to break down these pages first by character and then by token. To achieve this, we'll utilize some helpful utilities from LangChain, including LangChain text splitters. Specifically, we'll employ the recursive character text splitter and the sentence transformers token text splitter. The reason for choosing the sentence transformers token text splitter will be clarified shortly, but let's begin with the character splitter.

The character splitter divides text recursively based on certain divider characters. Practically, this means it initially identifies and splits the text at every occurrence of double newlines. If the resulting chunks are still larger than our target chunk size, which in this case is 1000 characters, it proceeds to split using the next designated character, continuing this process down to just a space, and ultimately, if necessary, splitting at character boundaries. We've also opted for a chunk overlap of 0. This overlap is a hyperparameter that allows for customization based on what is considered optimal chunking for specific needs.

Let's proceed with executing this step. We'll output the result of the character text splitter, focusing specifically on the 10th text split chunk we obtain. Additionally, we'll examine the total number of chunks generated by the character splitter. This process not only showcases the flexibility of text splitting techniques but also emphasizes the importance of tailored preprocessing in handling large text documents for retrieval systems.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter

```


```python
character_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""],
    chunk_size=1000,
    chunk_overlap=0
)
character_split_texts = character_splitter.split_text('\n\n'.join(pdf_texts))

print(word_wrap(character_split_texts[10]))
print(f"\nTotal chunks: {len(character_split_texts)}")
```

    increased, due in large part to significant global datacenter
    expansions and the growth in Xbox sales and usage. Despite 
    these
    ..
    launching new Circular Centers to increase reuse and reduce e -waste at
    our datacenters.  
    We contracted to protect over 17,000 acres of land
    (50% more than the land we use to operate), thus achieving our
    
    Total chunks: 347

We observed that the 10th chunk is a segment of text processed through the recursive character text splitter, resulting in a total of 347 chunks from the annual report PDF. However, splitting text by character alone is not sufficient due to the limitations of the embedding model we utilize, known as sentence transformers, which has a limited context window width of 256 characters. This limitation is crucial because the embedding model will typically truncate any characters or tokens that exceed its context window, potentially omitting important information.

To ensure we capture the full meaning in each chunk for embedding, it's essential to also split the text according to token count. We are using the Sentence Transformer text splitter for this purpose, with a chunk overlap of zero, and setting it to handle 256 tokens per chunk, matching the context window length of the Sentence Transformer embedding model. This step involves re-splitting all chunks generated by the character text splitter using the token text splitter. We aim to produce an output similar to the previous step to examine the effectiveness of this method in preserving the integrity of the information for embedding purposes.



```python
token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)

token_split_texts = []
for text in character_split_texts:
    token_split_texts += token_splitter.split_text(text)

print(word_wrap(token_split_texts[10]))
print(f"\nTotal chunks: {len(token_split_texts)}")
```

    increased, due in large part to significant global datacenter expansions and the growth in xbox sates and usage. despite these increases, we renain dedicated to achieving a net - zero future. we recognize that progress won ' $t$ always be linear, and the rate at which we can inplenent enissions reductions is dependent on many factors that can fluctuate over tine. on the path to beconing water positive, we imvested in 21 water replenishment projects that are expected to generate over 1.3 million cubic neters of volumetric benefits in nine water basins around the world. progress toward our zero waste connitment included diverting more than 15, 200 metric tons of solid waste otherwise headed to landfills and incinerators, as well as launching new circular centers to increase reuse and reduce $e$ - waste at our datacenters. we pontracted to protect over 17, 000 acres of land ( 50 s more than the land we use to operate), thus achieving our
    Totat chunks: 349

Observing the text, we find that the 10th chunk appears slightly different from what we had before, with fewer characters, limited to only 256 tokens. Interestingly, this process resulted in a slight increase in the number of chunks; from the 347 chunks previously identified, we now have 349. This indicates that a couple of existing chunks were divided further.

The process of chunking text is a foundational step in any retrieval augmented generation system. The subsequent phase involves loading these text chunks into our retrieval system, for which we will be utilizing Chroma. To integrate Chroma, we import it along with the sentence transformer embedding model.

Discussing the Sentence Transformer Embedding Model, it's an advanced version of the BERT transformer architecture, designed to embed each token individually, such as in the sequence "classifier instruction token" and "I like dogs." Each token is assigned its own dense vector embedding. 

<img src="/deeplearningai/advanced-retrieval-for-ai-chroma/images/Screenshot_2024-02-24_at_12.55.55 PM.png" width="80%" />


The innovation of Sentence Transformers lies in their ability to embed entire sentences or small documents by aggregating the output of all token embeddings into a single dense vector per document, or in our case, per chunk. This makes Sentence Transformers an excellent choice for embedding models due to their open-source nature, readily available weights online, and ease of local operation.

<img src="/deeplearningai/advanced-retrieval-for-ai-chroma/images/Screenshot_2024-02-24_at_12.56.20 PM.png" width="80%" />

Reference [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)

Integrated into Chroma, further information about Sentence Transformers can be found on their website or through their documentation. This elucidates our choice of the Sentence Transformer tokenizer text splitter, and we will proceed to create a Sentence Transformer embedding function to facilitate this process.

This is for use with Chroma, and we're going to demonstrate what happens when this embedding function is utilized. Let's take a look. You may receive a warning about HuggingFace tokenizers, which is a minor bug in HuggingFace but nothing to worry about and perfectly normal. The output we get is a very long, dense vector. Every entry in the vector has a number associated with it, and this represents the 10th text chunk that we showed you earlier as a dense vector. This vector has 358 dimensions, which might sound extensive unless you consider the full dimensionality of all English text, which is much higher.

```python
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

embedding_function = SentenceTransformerEmbeddingFunction()
print(embedding_function([token_split_texts[10]]))
```

The next step is to set up Chroma. We're going to use the default Chroma client, which is great for experimenting in a notebook. We'll make a new Chroma collection named "Microsoft Annual Report 2022" and pass in our previously defined embedding function, the sentence transformer embedding function. We are going to create IDs for each of the text chunks we've created, which will just be the string of the number of their position in the total token-split texts. Then, we'll add those documents to our Chroma collection. To ensure everything is as we expect, we'll output the count after everything has been added and run this cell.

```python
chroma_client = chromadb.Client()
chroma_collection = chroma_client.create_collection("microsoft_annual_report_2022", embedding_function=embedding_function)

ids = [str(i) for i in range(len(token_split_texts))]

chroma_collection.add(ids=ids, documents=token_split_texts)
chroma_collection.count()
```
Output will be listing below, which matches the count earlier

    349

So now that we have everything loaded into Chroma, let's connect an LLM and build a full-fledged RAG system. We're going to demonstrate how querying and retrieval and LLMs all work together. So let's start with a pretty simple query. If you're reading an annual financial report, one of the top questions you have in mind is, what was the total revenue for this year? We're going to get some results from Chroma by querying it. We pass our query texts and we're asking for five results. Chroma under the hood will use the embedding function that you've defined on your collection to automatically embed the query for you. You don't really have to do anything else to call that embedding function again.

We're going to pull the retrieved documents out of the results. This zero on the end is basically saying, give me the results for the zeroth query. We only have the one query. What we're gonna output now is basically the retrieved documents themselves and take a look. So let's run the cell. We see that the documents that we get here are fairly relevant to our query. What was the total revenue? We have classified revenue by different product and service offerings. We're talking about an unearned revenue, and there's more information in a similar vein.

The next step is to use these results together with an LLM to answer our query.

```python
query = "What was the total revenue?"

results = chroma_collection.query(query_texts=[query], n_results=5)
retrieved_documents = results['documents'][0]

for document in retrieved_documents:
    print(word_wrap(document))
    print('\n')

```

Output of the print above can be found at following [link](./1.1-output.html).


The next step is to use these results in conjunction with an LLM to address our query. We'll employ GPT for this purpose, necessitating a bit of initial setup to establish an OpenAI client. We'll retrieve our OpenAI API key from the environment for authentication purposes and proceed to create an OpenAI client. This process involves the utilization of their latest API, which conveniently encapsulates all functionalities within a single, streamlined client object. After running the necessary cell, we will not observe any immediate output, but rest assured, everything will be primed for use.


```python
import os
import openai
from openai import OpenAI

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
openai.api_key = os.environ['OPENAI_API_KEY']

openai_client = OpenAI()
```
We'll then proceed to define a function that facilitates communication with the model, incorporating our retrieved results and the initial query. Our choice of model for this task will be GPT 3.5 Turbo, known for its effectiveness in RAG loops and its rapid response time. The process begins by consolidating our retrieved documents into a cohesive string termed 'information', using a double newline as a separator. We'll also establish some prefatory messages. The system prompt is designed to guide the model's responses to our inputs, portraying it as a helpful expert and financial research assistant, equipped to answer questions pertaining to the contents of an annual report.

You'll be shown the user's question and the relevant information from in the annual report.
Answer the user's question using only this information.
So what this is doing, and this is really the core of the entire RAG loop, we're turning GPT from a model that remembers facts into a model that processes information.
That's the system prompt. And now we're going to add another piece of the message for our user content.
And we have here that we're in the role of the user, and here's the content. The content is essentially a formatted string that says, Here's our question, and that's just our original query.
Here's the information you're supposed to use. Here's the information.
And then we need to send the request to the OpenAl client, which is just using the normal API from the client. There's nothing special here at all. We're specifying the model. We're sending the messages. We're basically calling the chat completion endpoint on the OpenAl client, specifying a model and the messages we'd like to send and getting the response back.
And then we need to do a little bit more just to unpack the response from what the client returns.
So we've defined our RAG function and now let's actually use it. Let's put everything together.
So here's what we're going to do.
We are going to say output is equal to calling RAG with our query and retrieve documents. Then we're just gonna print the word wrap output and fire away.
There we go. The total revenue for the year ended June 30, 2022, was $198,270 million for Microsoft.
Microsoft is doing pretty well.
Now's a good time to take a moment and try some of your own queries.
So remember we specified the query a little bit further up. What was the total revenue? Try some of your own and see what the model outputs based on the retrieved results.



```python
def rag(query, retrieved_documents, model="gpt-3.5-turbo"):
    information = "\n\n".join(retrieved_documents)

    messages = [
        {
            "role": "system",
            "content": "You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report."
            "You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information."
        },
        {"role": "user", "content": f"Question: {query}. \n Information: {information}"}
    ]
    
    response = openai_client.chat.completions.create(
        model=model,
        messages=messages,
    )
    content = response.choices[0].message.content
    return content
```


```python
output = rag(query=query, retrieved_documents=retrieved_documents)

print(word_wrap(output))
```

Final output


```
The total revenue for the year ended June 30, 2022, was $198,270
million.
```
The model outputs based on the retrieved results from the annual report. It is really important to play with your retrieval system to gain intuition about what the model and the retriever can and can't do together before we dive into really analyzing how the system works. In the next lab, we're going to talk about some of the pitfalls and common failure modes of using retrieval in a retrieval augmented generation loop.