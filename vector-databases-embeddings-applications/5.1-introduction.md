---
layout: default
title: 5.1 Understand BM25
nav_order: 1
description: ".."
has_children: false
parent:  5. Sparse Dense and Hybrid Search
grand_parent:  Vector Databases
mathjax: true
---


<script type="text/javascript"
	src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,
	/javascripts/MathJaxLocal.js
"></script>

## Introduction

In this lesson, we'll introduce the concepts of sparse and dense search. Covering techniques for implementing both and discuss the advantages and disadvantages of each. We'll then introduce the practical and popular methodology of combining this using hybrid search. Hybrid search allows you to make the most of both search techniques and fusing the return rank results. Let's roll. 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.22.25 PM.png" width="80%" />

Let's see what's the difference between dense search and sparse search and why would you want 
one over the other. So, dense search uses vector embeddings representation of the data to perform the search. So, it relies on the meaning of the data in order 
to perform that query. So for example, if we look for baby dogs maybe we can get back information and content on puppies. However, this has its limitations. 

For example, if the model that we are using was trained on a completely different domain, the accuracy 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.22.53 PM.png" width="80%" />

of our queries would be rather poor. It's very much like if you went to a doctor and asked them how to 
fix a car engine. Well, the doctor probably wouldn't have a good answer for you. 
Another example is when we're dealing with stuff like serial numbers, like seemingly random strings of text. And in this case, also, there isn't a lot of meaning into codes like BB43300, right? 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.23.12 PM.png" width="80%" />

Like if you ask the semantic engine for finding content with that, you will get high quality results back. This is why we need to actually go into a different direction for situations like this and try to go for keyword search, also known as sparse search. 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.23.38 PM.png" width="80%" />

Sparse search is a way that allows you to utilize the keyword matching across all of your content. One example could be, hey, we could use bag of words. And the 
idea behind bag of words is like for every passage of text that you have in your data, what you 
can do is grab all the words and then keep adding and expanding to your table of available words, just like you see below. 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.24.05 PM.png" width="80%" />

So in this case, we can see that like maybe extremely, and cute appeared once in this sentence, 
and then word eat appears twice. So, that's how we can construct that for sparse embedding for this object. 
As I mentioned, this is called sparse embedding because if you have across all of your data, are so many words, actually, the vector that will represent that data will have a lot of slots where you could count 
each individual word. 

But in reality, you would be catching maybe 1% of available words. So, you'd have a lot of zeros in your data. 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.24.51 PM.png" width="80%" />


A good example of a keyword-based algorithm is Best Matching 25, also known as BM25. 

<img src="/deeplearningai/vector-databases-embeddings-applications/l5_images/Screenshot_2024-01-27_at_10.25.06 PM.png" width="80%" />

It actually performs really well when it comes to searching across many, many keywords. And the idea behind it is that 
it counts the number of words within the phrase that you are passing in and then those that appear more 
often are weighted as like less important when the match occurs but words that are rare if we match on that the score is a lot higher. And like you see this example here the sentence that we provided at the bottom will result in quite a lot of zeros that's why we call it sparse vector search. 

Refer to the next section for more details.

