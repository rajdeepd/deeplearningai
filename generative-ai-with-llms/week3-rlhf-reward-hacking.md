---
layout: default
title: RLHF- Reward hacking
nav_order: 7
description: "RLHF: Reward hacking"
has_children: false
parent: Week3
grand_parent: Coursera - GenAI with LLMs 
tags: [MathJax, Mathematic]
mathjax: true
---
# RLHF: Reward hacking

Let's recap what you've seen so far. Arlo HF is a fine-tuning process that aligns LLMs with human preferences. In this process, you make use of a reward model to assess and LLMs completions of a prompt data set against some human preference metric, like helpful or not helpful. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.40.13_PM.png" width="80%" />

Next, you use a reinforcement learning algorithm, in this case, PPO, to update the weights off the LLM based on the reward is signed to the completions generated by the current version off the LLM. You'll carry out this cycle of a multiple iterations using many different prompts and updates off the model weights until you obtain your desired degree of alignment. Your end result is a human aligned LLM that you can use in your application. An interesting problem that can emerge in reinforcement learning is known as reward hacking, where the agent learns to cheat the system by favoring actions that maximize the reward received even if those actions don't align well with the original objective.


## Potential problem: reward hacking

In the context of LLMs, reward hacking can manifest as the addition of words or phrases to completions that result in high scores for the metric being aligned. But that reduce the overall quality of the language. For example, suppose you are using RHF to detoxify and instruct model.

You have already trained a reward model that can carry out sentiment analysis and classify model completions as toxic or non-toxic. You select a prompt from the training data this product is, and pass it to the instruct an LLM which generates a completion. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.40.42_PM.png" width="80%" />

This one, complete garbage is not very nice and you can expect it to get a high toxic rating. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.40.57_PM.png" width="80%" />

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.52.35_PM.png" width="80%" />

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.52.50_PM.png" width="80%" />

The completion is processed by the toxicity of reward model, which generates a score and this is fed to the PPO algorithm, which uses it to update the model weights. As you iterate RHF will update the LLM to create a less toxic responses.

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.53.37_PM.png" width="80%" />

However, as the policy tries to optimize the reward, it can diverge too much from the initial language model. In this example, the model has started generating completions that it has learned will lead to very low toxicity scores by including phrases like most awesome, most incredible. This language sounds very exaggerated. The model could also start generating nonsensical, grammatically incorrect text that just happens to maximize the rewards in a similar way, outputs like this are definitely not very useful. 
## Avoiding reward hacking

To prevent our board hacking from happening, you can use the initial instruct LLM as performance reference.

Let's call it the reference model. The weights of the reference model are frozen and are not updated during iterations of RHF. This way, you always maintain a single reference model to compare to.

During training, each prompt is passed to both models, generating a completion by the reference LLM and the intermediate LLM updated model.



<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.53.50_PM.png" width="80%" />

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.54.13_PM.png" width="80%" />

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.55.17_PM.png" width="80%" />

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.55.44_PM.png" width="80%" />


 At this point, you can compare the two completions and calculate a value called the Kullback-Leibler divergence, or KL divergence for short. KL divergence is a statistical measure of how different two probability distributions are. You can use it to compare the completions off the two models and determine how much the updated model has diverged from the reference. Don't worry too much about the details of how this works. The KL divergence algorithm is included in many standard machine learning libraries and you can use it without knowing all the math behind it. You'll actually make use of KL divergence in this week's lab so you can see how this works for yourself. KL divergence is calculated for each generate a token across the whole vocabulary off the LLM. This can easily be tens or hundreds of thousands of tokens.

However, using a softmax function, you've reduced the number of probabilities to much less than the full vocabulary size. Keep in mind that this is still a relatively compute expensive process. You will almost always benefit from using GPUs.

## Evaluate the human-aligned LLM

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.56.15_PM.png" width="80%" />

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_10.56.27_PM.png" width="80%" />

Question: How can RLHF align the performance of large language models with human * preferences? 
Select all that apply
* RLHF can enhance the interpretability of generated text
* RLHF can help reduce model toxicity and misinformation
* RLHF increases the model's size by adding new parameters that represent human preferences
* Inference is faster after RLHF, improving the user experience