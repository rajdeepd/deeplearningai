---
layout: default
title: RLHF- Fine-tuning with reinforcement learning
nav_order: 6
description: "RLHF: Fine-tuning with reinforcement learning"
has_children: false
parent: Week3
grand_parent: Coursera - GenAI with LLMs 
tags: [MathJax, Mathematic]
mathjax: true
---
# RLHF: Fine-tuning with reinforcement learning

Let's bring everything together, and look at how you will use the reward model in the reinforcement learning process to update the LLM weights, and produce a human aligned model. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.30.28_AM.PNG" width="80%" />


Remember, you want to start with a model that already has good performance on your task of interests. You'll work to align an instruction finds you and LLM. First, you'll pass a prompt from your prompt dataset. In this case, a dog is, to the instruct LLM, which then generates a completion, in this case a furry animal. Next, you sent this completion, and the original prompt to the reward model as the prompt completion pair. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.30.37_AM.PNG" width="80%" />

The reward model evaluates the pair based on the human feedback it was trained on, and returns a reward value. A higher value such as `0.24`` as shown here represents a more aligned response. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.30.43_AM.PNG" width="80%" />

A less aligned response would receive a lower value, such as negative `0.53``. You'll then pass this reward value for the prom completion pair to the reinforcement learning algorithm to update the weights of the LLM, and move it towards generating more aligned, higher reward responses. Let's call this intermediate version of the model the RL updated LLM. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.30.50_AM.PNG" width="80%" />


These iterations continue for a given number of epics, similar to other types of fine tuning. Here you can see that the completion generated by the RL updated LLM receives a higher reward score, indicating that the updates to weights have resulted in a more aligned completion. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.31.34_AM.PNG" width="80%" />

If the process is working well, you'll see the reward improving after each iteration as the model produces text that is increasingly aligned with human preferences. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.43.19_AM.png" width="80%" />


You will continue this iterative process until your model is aligned based on some evaluation criteria. For example, reaching a threshold value for the helpfulness you defined. You can also define a maximum number of steps, for example, 20,000 as the stopping criteria. 

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.43.27_AM.png" width="80%" />
<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.43.37_AM.png" width="80%" />

At this point, let's refer to the fine-tuned model as the human-aligned LLM. One detail we haven't discussed yet is the exact nature of the reinforcement learning algorithm. This is the algorithm that takes the output of the reward model and uses it to update the LLM model weights so that the reward score increases over time.

<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.44.40_AM.png" width="80%" />

There are several different algorithms that you can use for this part of the RLHF process. 


<img src="/deeplearningai/generative-ai-with-llms/images/Screenshot_2023-09-23_at_7.45.27_AM.png" width="80%" />

A popular choice is proximal policy optimization or PPO for short. PPO is a pretty complicated algorithm, and you don't have to be familiar with all of the details to be able to make use of it. However, it can be a tricky algorithm to implement and understanding its inner workings in more detail can help you troubleshoot if you're having problems getting it to work. To explain how the PPO algorithm works in more detail, I invited my AWS colleague, Ek to give you a deeper dive on the technical details. This next video is optional and you should feel free to skip it, and move on to the reward hacking video. You won't need the information here to complete the quizzes or this week's lab. However, I encourage you to check out the details as RLHF is becoming increasingly important to ensure that LLMs behave in a safe and aligned manner in deployment.
