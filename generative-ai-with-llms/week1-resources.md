---
layout: default
title: Week 1 Resources
nav_order: 16
has_children: false
parent: Week1
grand_parent: Coursera - GenAI with LLMs 
---

Model architectures and pre-training objectives
What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?
 - The paper examines modeling choices in large pre-trained language models and identifies the optimal approach for zero-shot generalization.

HuggingFace Tasks
 and 
Model Hub
 - Collection of resources to tackle varying machine learning tasks using the HuggingFace library.

LLaMA: Open and Efficient Foundation Language Models
 - Article from Meta AI proposing Efficient LLMs (their model with 13B parameters outperform GPT3 with 175B parameters on most benchmarks)

Scaling laws and compute-optimal models
Language Models are Few-Shot Learners
 - This paper investigates the potential of few-shot learning in Large Language Models.

Training Compute-Optimal Large Language Models
 - Study from DeepMind to evaluate the optimal model size and number of tokens for training LLMs. Also known as “Chinchilla Paper”.

BloombergGPT: A Large Language Model for Finance
 - LLM trained specifically for the finance domain, a good example that tried to follow chinchilla laws.

