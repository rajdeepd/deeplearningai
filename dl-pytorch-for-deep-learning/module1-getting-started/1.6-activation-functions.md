---
layout: default
title: 1-6 Activation Functions
nav_order: 6
description: "Simple Neural network"
has_children: false
parent: DL - Pytorch for Deep Learning
---

### The Problem: When Straight Lines Fail

In the previous lab, you saw that a simple **linear model** works for bike deliveries, where the relationship between distance and time is a straight line.

However, when you added car data, the model failed. Why? Because the real world isn't always linear.

  <img src="./images/af-fig1.png"/>

  * **Short distances (city):** Dense traffic makes these miles slow.
  * **Long distances (highway):** Cars move much faster.

The relationship isn't a line; it's a **curve**.

  <img src="./images/af-fig2.png"/>

### The Failed Solution: Stacking More Linear Neurons

You might think, "If one neuron isn't enough, let's add more\!" You could build a model like this:

  <img src="./images/af-fig3.png"/>

1.  **Input Layer:** 1 feature (distance)
2.  **Hidden Layer:** 2 neurons
3.  **Output Layer:** 1 neuron (to combine the hidden layer's outputs)

  <img src="./images/af-fig4.png"/>

  <img src="./images/af-fig5.png"/>

  <img src="./images/af-fig6.png"/>

**This will not work.**

A linear combination of linear functions is *still* just a linear function. No matter how many linear layers or neurons you stack, you will always end up with a straight line.

### The Real Solution: Non-Linear Activation Functions

To learn curves, your model needs a new ingredient: a **non-linear activation function**.



This is a simple function that transforms the output of a neuron. The flow for a single neuron changes from:

  * **Linear:** `output = (W * x) + b`
  * **Non-Linear:** `z = (W * x) + b` $\rightarrow$ `output = activation_function(z)`

This small change is what allows a model to learn complex, non-linear patterns.

  <img src="./images/af-fig7.png"/>

-----

### Meet ReLU: The Most Important Activation Function

The most common and powerful activation function is **ReLU (Rectified Linear Unit)**. Its rule is incredibly simple:

>   * If the input is negative, the output is **zero**.
>   * If the input is positive, the output is the **input (unchanged)**.

  <img src="./images/af-fig8.png"/>

### How ReLU Creates a "Bend"

Let's see what happens when we apply ReLU to our single neuron:

1.  The neuron calculates its linear output: `z = (W * distance) + b`
2.  ReLU is applied to `z`.

<!-- end list -->

  * If `(W * distance) + b` is negative, the output is `0` (a flat line).
  * If `(W * distance) + b` is positive, the output is `(W * distance) + b` (a sloped line).

The result is no longer a single straight line\! You now have a **bend** or a "corner" where the model's behavior changes.

By learning the `W` (weight) and `B` (bias) values, the neuron learns *where to place this bend*. The bend occurs where `(W * x) + b = 0`.

  <img src="./images/af-fig9.png"/>

 

-----

### From One Bend to a Complex Curve

Your delivery data doesn't just have one bend; it has a complex curve. So, if one neuron with ReLU gives you *one bend*, what do multiple neurons give you?

  <img src="./images/af-fig10.png"/>

  <img src="./images/af-fig11.png"/>

**Multiple bends\!**

You can build a hidden layer with several neurons (e.g., three). Each neuron:

  * Receives the same `distance` input.
  * Has its *own* unique `W` and `B`.
  * Learns to place its "bend" at a different location.

 <img src="./images/af-fig12.png"/>

For example:

  * **Neuron 1** activates at 3 miles (where city traffic starts).
  * **Neuron 2** activates at 8 miles (where the highway begins).
  * **Neuron 3** activates at 15 miles (full highway speed).

When the output layer adds these three "bent lines" together, their combined shape can approximate your complex delivery curve.

 <img src="./images/af-fig13.png"/>

### The PyTorch Code

This entire model is simple to build in PyTorch:

```python
model = nn.Sequential(
    # 1. Hidden Layer
    nn.Linear(in_features=1, out_features=3),
    nn.ReLU(),
    
    # 2. Output Layer
    nn.Linear(in_features=3, out_features=1)
)
```

1.  `nn.Linear(1, 3)`: The hidden layer. It takes **1** input (distance) and has **3** neurons.
    <img src="./images/af-fig14.png"/>

2.  `nn.ReLU()`: Applies the ReLU function to the outputs of all three neurons.

    <img src="./images/af-fig15.png"/>

    <img src="./images/af-fig16.png"/>

    <img src="./images/af-fig17.png"/>

    <img src="./images/af-fig18.png"/>

    <img src="./images/af-fig19.png"/>

3.  `nn.Linear(3, 1)`: The output layer. It takes the **3** transformed values from the ReLU layer and combines them into **1** final prediction (delivery time).

    
     <img src="./images/af-fig20.png"/>

If you want an even smoother, more accurate curve, you just increase the number of neurons (e.g., `nn.Linear(1, 10)`).


### Other Activation Functions

While ReLU is the workhorse of modern deep learning, others exist:

<img src="./images/af-fig21.png"/>

  * **Sigmoid:** Squashes values into a range between 0 and 1. Great for probabilities.
  * **Tanh (Hyperbolic Tangent):** Squashes values into a range between -1 and 1.

<img src="./images/af-fig23.png"/>

For most problems, however, **ReLU is all you need.** 

<img src="./images/af-fig24.png"/>

Now you're ready to go back to the lab and conquer that curved data which we will cover in the next section.

